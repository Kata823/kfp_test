{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f10913-165d-4192-9f41-3447f714569b",
   "metadata": {},
   "source": [
    "# Vertex Pipelines: Qwik Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16420b30-8890-4bf6-a5ee-3735057aee15",
   "metadata": {},
   "source": [
    "## Vertex Pipelines settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75631f16-d390-4de8-98f8-c7fdf862ce4d",
   "metadata": {},
   "source": [
    "There are a few additional libraries you'll need to install in order to use Vertex Pipelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2c6fe-5926-47e8-b931-f866b00c0c96",
   "metadata": {},
   "source": [
    "- `Kubeflow Pipelines`: This is the SDK used to build the pipeline. Vertex Pipelines supports running pipelines built with both Kubeflow Pipelines or TFX.\n",
    "- `Google Cloud Pipeline Components`: This library provides pre-built components that make it easier to interact with Vertex AI services from your pipeline steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36420094-f539-4756-8320-d51ea3cf0113",
   "metadata": {},
   "source": [
    "### Step 1: Create Python notebook and install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b63c21-a658-482e-83f2-d8a11ca9b0a3",
   "metadata": {},
   "source": [
    "From the Launcher menu in your Notebook instance, create a notebook by selecting Python 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58085175-4f19-455e-b5c3-17928c363329",
   "metadata": {},
   "source": [
    "<img src=\"img/GCP_python.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83b9b1-9e4f-453e-bd0e-77bccce7274c",
   "metadata": {},
   "source": [
    "You can access the Launcher menu by clicking on the + sign in the top left of your notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2908d95-f4eb-49ef-ae84-0adb23e54b9a",
   "metadata": {},
   "source": [
    "To install both services needed for this lab, first set the user flag in a notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585960dc-fd28-4954-9343-e87d8b001cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb4bd04-3639-43ac-8558-2a9bc273caf3",
   "metadata": {},
   "source": [
    "Then run the following from your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4766ebb-8f12-4870-ba3e-1afcc0c967c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform==1.0.0\n",
      "  Downloading google_cloud_aiplatform-1.0.0-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.0.0) (1.34.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.0.0) (1.22.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform==1.0.0) (23.1)\n",
      "Collecting google-cloud-storage<2.0.0dev,>=1.32.0 (from google-cloud-aiplatform==1.0.0)\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-bigquery<3.0.0dev,>=1.15.0 (from google-cloud-aiplatform==1.0.0)\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.59.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.20.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2.20.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.5.0)\n",
      "Collecting packaging>=14.3 (from google-cloud-aiplatform==1.0.0)\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<2.0.0dev,>=1.32.0->google-cloud-aiplatform==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.0.0) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.26.16)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2023.5.7)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (0.5.0)\n",
      "Installing collected packages: packaging, google-cloud-storage, google-cloud-bigquery, google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.2.0 requires pandas!=1.4.0,<2,>1.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.0.0 google-cloud-bigquery-2.34.4 google-cloud-storage-1.44.0 packaging-21.3\n",
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.10/site-packages (1.8.22)\n",
      "Collecting kfp\n",
      "  Downloading kfp-2.6.0.tar.gz (425 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.3/425.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-pipeline-components==0.1.1\n",
      "  Downloading google_cloud_pipeline_components-0.1.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.0.0 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-pipeline-components==0.1.1) (1.0.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.4.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=1.20.0 in /home/jupyter/.local/lib/python3.10/site-packages (from kfp) (1.44.0)\n",
      "Requirement already satisfied: kubernetes<26,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (25.3.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.20.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.2.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.8.5)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from kfp) (4.17.3)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.3)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.2.14)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.15)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.16 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.1.16)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.5.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (3.20.3)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: urllib3<2 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.16)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.10.9)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.15.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire<1,>=0.3.1->kfp) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire<1,>=0.3.1->kfp) (2.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.59.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.31.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (1.22.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (2.34.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=1.20.0->kfp) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=1.20.0->kfp) (2.5.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5,>=3.0.1->kfp) (0.19.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp) (67.7.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp) (1.5.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<26,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1.8.2->kfp) (4.5.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.40.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.48.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<3,>=1.20.0->kfp) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client<2,>=1.7.8->kfp) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<26,>=8.0.0->kfp) (3.2.2)\n",
      "Installing collected packages: google-cloud-pipeline-components\n",
      "Successfully installed google-cloud-pipeline-components-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42611db-a84c-4806-9d29-045f851c4677",
   "metadata": {},
   "source": [
    "After installing these packages you'll need to restart the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774b8439-1112-4eed-af7a-76ca97bf2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # インストール後カーネルを自動で再起動\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b914f61-fad2-42d5-9918-92c954bce7d3",
   "metadata": {},
   "source": [
    "Finally, check that you have correctly installed the packages. The KFP SDK version should be >=1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05e0687-46fc-40bc-bf43-bf678e6e3182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.22\n",
      "google_cloud_pipeline_components version: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853cee4d-6d9c-4a21-8948-f0510e8f65f4",
   "metadata": {},
   "source": [
    "### Step 2: Set your project ID and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e70116-03be-41c3-abac-c527a17fa309",
   "metadata": {},
   "source": [
    "Throughout this training you'll reference your Cloud Project ID and the bucket you created earlier. Next you'll create variables for each of those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df416486-827a-415a-9dcd-bbb32d979580",
   "metadata": {},
   "source": [
    "Then create a variable to store your bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcd7800-9150-430d-85d1-7dbaf2991fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ccbd-ecbdp-bds\"\n",
    "BUCKET_NAME = \"gs://ccbd-bds-aa-kfp-test2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315861d5-a8e0-4eda-9ff5-6f5da3710064",
   "metadata": {},
   "source": [
    "### Step 3: Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841db37-d3f5-42df-a654-c9e0b600eb33",
   "metadata": {},
   "source": [
    "Add the following to import the libraries you'll be using throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20090bba-9063-45ab-88f8-ced59bc466d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a42b3a-d90e-4277-9e58-0bae8a09e873",
   "metadata": {},
   "source": [
    "### Step 4: Define constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084cf94-a7ed-4f7d-b3b6-e479f75e772d",
   "metadata": {},
   "source": [
    "The last thing you need to do before building the pipeline is define some constant variables. `PIPELINE_ROOT` is the Cloud Storage path where the artifacts created by your pipeline will be written. You're using `asia-northeast1`. as the region here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e0f08b-d4f2-4cf9-8371-8cd32f6d8d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n",
      "gs://ccbd-bds-aa-kfp-test2/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "REGION=\"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "print(PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2108772-8343-4492-abc3-524331d64e56",
   "metadata": {},
   "source": [
    "After running the code above, you should see the root directory for your pipeline printed. This is the Cloud Storage location where the artifacts from your pipeline will be written. It will be in the format of `gs://<bucket_name>/pipeline_root/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57430b33-46d9-4ae4-acbb-139b59e84533",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating an end-to-end ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a17aa-588a-4ae0-b730-454a13f26eb6",
   "metadata": {},
   "source": [
    "It's time to build your first ML pipeline. In this pipeline, you'll use the UCI Machine Learning Dry Beans dataset, from: KOKLU, M. and OZKAN, I.A., (2020), \"Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.\"In Computers and Electronics in Agriculture, 174, 105507. DOI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84781231-30ef-4f31-8d6a-316b168e239b",
   "metadata": {},
   "source": [
    "<font color=\"CornflowerBlue\">**Note**: This pipeline will take over 2 hours to complete. So you will not need to wait the entire duration of the pipeline to complete the lab. Follow the steps until your pipeline job has started.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b44804-382d-4fd2-b521-ff4106a72c3d",
   "metadata": {},
   "source": [
    "This is a tabular dataset, and in your pipeline you'll use the dataset to train, evaluate, and deploy an AutoML model that classifies beans into one of 7 types based on their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8d4e9-d707-41d0-a55a-0777289c7a81",
   "metadata": {},
   "source": [
    "This pipeline will:\n",
    "\n",
    "- Create a Dataset in Vertex AI\n",
    "- Train a tabular classification model with AutoML\n",
    "- Get evaluation metrics on this model\n",
    "- Based on the evaluation metrics, decide whether to deploy the model using conditional logic in Vertex Pipelines\n",
    "- Deploy the model to an endpoint using Vertex Prediction\n",
    "\n",
    "Each of the steps outlined will be a component. Most of the pipeline steps will use pre-built components for Vertex AI services via the `google_cloud_pipeline_components` library you imported earlier in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420c1f3-fa72-455f-9ebb-27f01c4d4274",
   "metadata": {},
   "source": [
    "In this section, we'll define one custom component first. Then, we'll define the rest of the pipeline steps using pre-built components. Pre-built components make it easier to access Vertex AI services, like model training and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01b52f-142a-4394-8382-3f36fb121a41",
   "metadata": {},
   "source": [
    "The majority of time for this step is for the AutoML training piece of this pipeline, which will take about an hour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6a1c9-bf83-49f8-aab6-aa0d1657e574",
   "metadata": {},
   "source": [
    "### Step 1: A custom component for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17740c0-bbdc-41db-b1ab-ec8d2896c52b",
   "metadata": {},
   "source": [
    "The custom component you'll define will be used towards the end of the pipeline once model training has completed. This component will do a few things:\n",
    "\n",
    "- Get the evaluation metrics from the trained AutoML classification model\n",
    "- Parse the metrics and render them in the Vertex Pipelines UI\n",
    "- Compare the metrics to a threshold to determine whether the model should be deployed\n",
    "\n",
    "Before defining the component, understand its input and output parameters. As input, this pipeline takes some metadata on your Cloud project, the resulting trained model (you'll define this component later), the model's evaluation metrics, and a `thresholds_dict_str`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180ac39-9947-4339-ae4f-ed52a31e213c",
   "metadata": {},
   "source": [
    "The `thresholds_dict_str` is something you'll define when you run your pipeline. In the case of this classification model, this will be the area under the ROC curve value for which you should deploy the model. For example, if you pass in 0.95, that means you'd only like your pipeline to deploy the model if this metric is above 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9060d4e-4cdf-4f8e-ab30-0ed0f7ac6020",
   "metadata": {},
   "source": [
    "The evaluation component returns a string indicating whether or not to deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de74f2-4d21-43d8-b27d-da9b6c5d1c6f",
   "metadata": {},
   "source": [
    "- Add the following in a notebook cell to create this custom component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477902fb-9846-4994-825c-e91cbdfc15cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"tables_eval_component.yaml\", # Optional: you can use this to load the component later\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classif_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"region\",\n",
    "    api_endpoint: str,  # \"region-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "\n",
    "    \"\"\"これは AutoML の表形式分類モデルの評価指標をレンダリングする関数である。\n",
    "    AutoML の表形式トレーニング プロセスによって生成された分類モデル評価を取得し、解析作業を行って、その情報を元にモデルの ROC 曲線と混同行列をレンダリングする。また与えられた指標のしきい値情報を用いて、評価結果と比較を行い、モデルの精度がデプロイするのに十分かどうかを判断する。\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import logging\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    # モデルの評価情報をフェッチ\n",
    "    def get_eval_info(client, model_name):\n",
    "        from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        for evaluation in response:\n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            for metric in metrics.keys():\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "\n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list,\n",
    "        )\n",
    "\n",
    "    # 与えられた指標のしきい値を用いてモデルの精度がデプロイするのに\n",
    "    # 十分かどうかを判断。\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        for k, v in thresholds_dict.items():\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # より高い方が良い\n",
    "                if metrics_dict[k] < v:  # しきい値を下回る場合はデプロイしない\n",
    "                    logging.info(\n",
    "                        \"{} < {}; returning False\".format(metrics_dict[k], v)\n",
    "                    )\n",
    "                    return False\n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        return True\n",
    "\n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "\n",
    "        # ROC 曲線をロギング\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "\n",
    "        # 混同行列をロギング\n",
    "        annotations = []\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\n",
    "            annotations.append(item[\"displayName\"])\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "\n",
    "        # テキストの指標情報もロギング\n",
    "        for metric in metrics_list[0].keys():\n",
    "            if metric != \"confidenceMetrics\":\n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aiplatform.init(project=project, location=location)\n",
    "    # モデルのリソース名を入力 Model Artifact から抽出\n",
    "    model_resource_path = model.uri.replace(\"aiplatform://v1/\", \"\")\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # リクエストの作成と送信に使うクライアントを初期化する。\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\n",
    "        client, model_resource_path\n",
    "    )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    if deploy:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "\n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b45c65-668c-440e-b059-84b79e4a9f20",
   "metadata": {},
   "source": [
    "### Step 2: Adding Google Cloud pre-built components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d4555-37e5-4031-968d-c40511398eed",
   "metadata": {},
   "source": [
    "In this step you'll define the rest of your pipeline components and see how they all fit together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f502928-80e0-456f-aece-cb8436129b71",
   "metadata": {},
   "source": [
    "1. First, define the display name for your pipeline run using a timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d8ac11-961f-42db-a0ef-af0b1a22b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-beans1705385628\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "DISPLAY_NAME = 'automl-beans{}'.format(str(int(time.time())))\n",
    "print(DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd565915-42d4-4e04-aee0-ca3ef446f7db",
   "metadata": {},
   "source": [
    "2. Then copy the following into a new notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9d39c5-ca79-4773-8d0f-33096adfdf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-beans-training-v2\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = \"bq://ccbd-ecbdp-bds.kfp_test2.beans1\",\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    api_endpoint: str = f\"{REGION}-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=display_name, bq_source=bq_source\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        location=gcp_region,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}},\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\",\n",
    "    )\n",
    "    model_eval_task = classif_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        training_op.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    with dsl.Condition(\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "    ):\n",
    "\n",
    "        deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            machine_type=\"e2-standard-4\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb3539-c2a6-497a-9a99-a48564f9f820",
   "metadata": {},
   "source": [
    "What's happening in this code:\n",
    "\n",
    "- First, just as in the previous pipeline, you define the input parameters this pipeline takes. You need to set these manually since they don't depend on the output of other steps in the pipeline.\n",
    "- The rest of the pipeline uses a few pre-built components for interacting with Vertex AI services:\n",
    "    - `TabularDatasetCreateOp` creates a tabular dataset in Vertex AI given a dataset source either in Cloud Storage or BigQuery. In this pipeline, you're passing the data via a BigQuery table URL.\n",
    "    - `AutoMLTabularTrainingJobRunOp` kicks off an AutoML training job for a tabular dataset. You pass a few configuration parameters to this component, including the model type (in this case, classification), some data on the columns, how long you'd like to run training for, and a pointer to the dataset. Notice that to pass in the dataset to this component, you're providing the output of the previous component via `dataset_create_op.outputs[\"dataset\"]` .\n",
    "    - ModelDeployOp deploys a given model to an endpoint in Vertex AI. There are additional configuration options available, but here you're providing the endpoint machine type, project, and model you'd like to deploy. You're passing in the model by accessing the outputs of the training step in your pipeline.\n",
    "- This pipeline also makes use of **conditional logic**, a feature of Vertex Pipelines that lets you define a condition, along with different branches based on the result of that condition. Remember that when you defined the pipeline you passed a `thresholds_dict_str` parameter. This is the accuracy threshold you're using to determine whether to deploy your model to an endpoint. To implement this, make use of the `Condition` class from the KFP SDK. The condition passed in is the output of the custom eval component you defined earlier in this lab. If this condition is true, the pipeline will continue to execute the `deploy_op` component. If accuracy doesn't meet the predefined threshold, the pipeline will stop here and won't deploy a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76dc2c-3b39-4a4c-9c69-9262e1058bb0",
   "metadata": {},
   "source": [
    "### Step 3: Compile and run the end-to-end ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a7d7d-3ecd-4a36-a99b-a7d997953239",
   "metadata": {},
   "source": [
    "1. With your pipeline defined, you're ready to compile it. The following will generate a JSON file that you'll use to run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bf0c2d-7096-40c0-84d8-322acb30ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"tab_classif_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a494a3e-38d1-46af-a09e-01aed0277a35",
   "metadata": {},
   "source": [
    "2. Next, instantiate an API client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7755b334-fd56-4887-87ab-85417bbde0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/v2/google/client/client.py:169: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcccd0-d286-4a3e-882c-345d7bff5925",
   "metadata": {},
   "source": [
    "3. Finally, kick off a pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d71328-055f-42fb-a8ce-7131ec87e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:googleapiclient.discovery:URL being requested: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/ccbd-ecbdp-bds/locations/us-central1/pipelineJobs?pipelineJobId=automl-tab-beans-training-v2-20240116061418&alt=json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-tab-beans-training-v2-20240116061418?project=ccbd-ecbdp-bds\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"tab_classif_pipeline.json\", pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID,\"display_name\": DISPLAY_NAME}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb03556-bd11-467c-a941-4ba343143a49",
   "metadata": {},
   "source": [
    "4. Click on the link shown after running the cell above to see your pipeline in the console. **This pipeline will take a little over an hour to run**. Most of the time is spent in the AutoML training step. The completed pipeline will look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fde5f7-d8f3-4c65-b6a5-3f6917dd6c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c57f4f2-2209-4032-9a2a-e7fa480a27d6",
   "metadata": {},
   "source": [
    "5. If you toggle the \"Expand artifacts\" button at the top, you'll be able to see details for the different artifacts created from your pipeline. For example, if you click on the `dataset` artifact, you'll see details on the Vertex AI dataset that was created. You can click the link here to go to the page for that dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d3ff0-2c33-43d1-8720-49625e37d935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1bea61-c601-41f1-a0fa-cc817808a514",
   "metadata": {},
   "source": [
    "6. Similarly, to see the resulting metric visualizations from your custom evaluation component, click on the artifact called **metricsc**. On the right side of your dashboard, you'll be able to see the confusion matrix for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670eed2-9677-4fdd-9f8d-cdcba350816e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a21ace62-d606-4a1b-9cb6-6e472f94529d",
   "metadata": {},
   "source": [
    "7. To see the model and endpoint created from this pipeline run, go to the models section and click on the model named `automl-beans`. There you should see this model deployed to an endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918533c-9061-4ab9-91b8-fc44223995be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95445089-dc58-4259-bb18-e0c875e2c0b0",
   "metadata": {},
   "source": [
    "8. You can also access this page by clicking on the **endpoint** artifact in your pipeline graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c7db1-e22e-447d-930d-b3f74b2abe17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7ed72e-5358-4515-b06f-83f3b756132b",
   "metadata": {},
   "source": [
    "9. In addition to looking at the pipeline graph in the console, you can also use Vertex Pipelines for **Lineage Tracking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f9637-953b-4245-b23b-0c88d3401c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc84a98d-a584-4637-a823-6d693499bc51",
   "metadata": {},
   "source": [
    "10. Lineage tracking means tracking artifacts created throughout your pipeline. This can help you understand where artifacts were created and how they are being used throughout an ML workflow. For example, to see the lineage tracking for the dataset created in this pipeline, click on the dataset artifact and then **View Lineage**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7f7e2-f39c-4391-b64f-ccc5c921900d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8a21820-66ff-4fd9-ac83-526bd6093322",
   "metadata": {},
   "source": [
    "This shows all the places this artifact is being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc83bfe-1206-415b-8969-a1c91fc87e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62ff44fb-c68f-4f14-93de-98b9ce3beeae",
   "metadata": {},
   "source": [
    "<font color=\"CornflowerBlue\">**Note**: Wait until the training job in your pipeline has started and then check your progress below. The training job will take longer than the time allotted for this lab but you will be awarded full points for submitting the training job successfully.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc4905-c962-46c3-8f9d-6965ff69c852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec6aec70-4e76-4380-937c-8968a36378a4",
   "metadata": {},
   "source": [
    "### Step 4: Comparing metrics across pipeline runs (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4bc3d-b064-4add-bd7b-ea1457514ebe",
   "metadata": {},
   "source": [
    "- If you run this pipeline multiple times, you may want to compare metrics across runs. You can use the `aiplatform.get_pipeline_df()` method to access run metadata. Here, we'll get metadata for all runs of this pipeline and load it into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911316da-52ea-421d-94e4-37f3ddb08938",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=\"automl-tab-beans-training-v2\")\n",
    "small_pipeline_df = pipeline_df.head(2)\n",
    "small_pipeline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1c496f-e92b-43f6-aa80-9fd3c01452f1",
   "metadata": {},
   "source": [
    "You've now learned how to build, run, and get metadata for an end-to-end ML pipeline on Vertex Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c54ba-abf8-4a08-aa5a-f44673891c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70943c8-e5a0-41e0-8636-4be9144aaf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
